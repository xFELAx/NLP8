{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Atencja — implementacja atencji między danym stanem dekodera a stanami kodera\n",
    "\n",
    "Poniżej znajdują się dwie macierze `encoder_states` i `decoder_states` reprezentujące stan warstwy ukrytej po przetworzeniu każdego słowa przez koder oraz statyczne wektory zanurzeń związane z danym wejściem dekodera. Pojedynczy stan warstwy ukrytej zawiera zanurzenia o długości = 3, która to liczba jest równa rozmiarowi zanurzenia w dekoderze. W koderze mamy 4 stany warstw ukrytych, ponieważ przetwarzamy sekwencję składającą się z 4 tokenów.\n",
    "\n",
    "W dekoderze znajduje się 5 tokenów, które są generowane na podstawie sekwencji przetwarzanej przez koder.\n",
    "\n",
    "*Zadanie (1 punkt)*\n",
    "\n",
    "Zadanie polega na: a) Obliczeniu podobieństwa wszystkich zanurzeń z dekodera (zapytań -- queries) do wszystkich zanurzeń kolejnych stanów kodera (kluczy -- keys) (pamiętaj, że macierze można transponować. W NumPy transponujemy macierz za pomocą `nazwa_macierzy. T`)\n",
    "\n",
    "a) Softmax (zaimportowany z scipy) należy wykonać na utworzonej macierzy podobieństw. Uwaga: pamiętaj, aby zastosować softmax we właściwym wymiarze. Wszystkie ukryte stany kodera powinny być zmiękczone w kontekście danego stanu dekodera. W scipy funkcja softmax zawiera argument `axis`, który może pomóc.\n",
    "\n",
    "b) Połącz macierz uwagi z kroku b) i „encoder_states”, aby wygenerować macierz zawierającą wektory kontekstu dla każdego tokena z dekodera."
   ],
   "metadata": {
    "id": "mooJ5vGuciH3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# scipy.special.softmax(x, axis=None)\n",
    "\n",
    "encoder_states = np.array(\n",
    "    [[1.2, 3.4, 5.6],  # encoder's hidden layer output at the step 1, related to a given input token, e.g., I\n",
    "     [-2.3, 0.2, 7.2],  # encoder's hidden layer output at the step 2, related to a given token, e.g., like\n",
    "     [10.2, 0.2, 0.3],  # encoder's hidden layer output at the step 3, related to a given token, e.g., NLP\n",
    "     [0.4, 0.7, 1.2]]  # encoder's hidden layer output at the step 4, related to a given token, e.g., \".\"\n",
    ")\n",
    "\n",
    "decoder_states = np.array(\n",
    "    [[0.74, 0.23, 0.56],  # decoder's static word embedding at the step 1, related to a given token, e.g., <BOS>\n",
    "     [7.23, 0.12, 0.55],  # decoder's static word embedding at the step 2, related to a given token, e.g., Ja\n",
    "     [9.12, 4.23, 0.44],  # decoder's static word embedding at the step 3, related to a given token, e.g., lubię\n",
    "     [4.1, 3.23, 0.5],  # decoder's static word embedding at the step 4, related to a given token, e.g., przetwarzanie\n",
    "     [5.2, 3.1, 8.5]]  # decoder's static word embedding at the step 5, related to a given token, e.g., języka\n",
    ")\n",
    "\n",
    "similarities = np.dot(decoder_states, encoder_states.T)\n",
    "attention_weights = softmax(similarities, axis=1)\n",
    "context_vectors = np.dot(attention_weights, encoder_states)\n",
    "\n",
    "print(similarities)\n",
    "print(attention_weights)\n",
    "print(context_vectors)"
   ],
   "metadata": {
    "id": "Vbq0QW2td41r",
    "ExecuteTime": {
     "end_time": "2023-04-30T10:35:33.187423900Z",
     "start_time": "2023-04-30T10:35:33.025318500Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.806   2.376   7.762   1.129]\n",
      " [ 12.164 -12.645  73.935   3.636]\n",
      " [ 27.79  -16.962  94.002   7.137]\n",
      " [ 18.702  -5.184  42.616   4.501]\n",
      " [ 64.38   49.86   56.21   14.45 ]]\n",
      "[[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03]\n",
      " [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31]\n",
      " [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38]\n",
      " [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17]\n",
      " [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]]\n",
      "[[ 9.69108631  0.35799187  0.59163688]\n",
      " [10.2         0.2         0.3       ]\n",
      " [10.2         0.2         0.3       ]\n",
      " [10.2         0.2         0.3       ]\n",
      " [ 1.20254471  3.39909302  5.59850122]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Oczekiwane wyjścia:\n",
    "\n",
    "a) [[ 4.806 2.376 7.762 1.129] [ 12.164 -12.645 73.935 3.636] [ 27.79 -16.962 94.002 7.137] [ 18.702 -5.184 42.616 4.501] [ 64.38 49.86 56.21 14.45 ]]\n",
    "\n",
    "b) [[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03] [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31] [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38] [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17] [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]]\n",
    "\n",
    "c) [[ 9.69108631 0.35799187 0.59163688] [10.2 0.2 0.3 ] [10.2 0.2 0.3 ] [10.2 0.2 0.3 ] [ 1.20254471 3.39909302 5.59850122]]\n"
   ],
   "metadata": {
    "id": "N8bfd5X4fAJq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer\n",
    "## Wykorzystanie modelu T5 opartego na transformerze do rozwiązywania różnych zadań NLP.\n",
    "\n",
    "Dzisiaj poznamy nową bibliotekę — HuggingFace **transformers** (https://huggingface.co/docs/transformers/index) i użyjemy jej do rozwiązania kilku nieoczywistych problemów związanych z NLP za pomocą modelu **T5** .\n",
    "\n",
    "\n",
    "HuggingFace transformers to jedna z najpopularniejszych bibliotek dostarczających nam wysokopoziomowe API do wykorzystania sieci neuronowych do rozwiązywania zadań związanych z przetwarzaniem języka naturalnego, przetwarzaniem dźwięku, wizją komputerową, a nawet scenariuszami multimodalnymi, w których musimy wykorzystywać wiele modalności naraz (np. odpowiadając na pytania o zdjęcia, wydobywając informacje z faktur).\n",
    "\n",
    "Najpierw zainstalujmy zależności, samą bibliotekę `transformers` oraz moduł `sentencepiece`, który pomaga nam tokenizować dokumenty i przekształcać tokeny w kodowanie one-hot (ideę sentencepiece omówimy szczegółowo później).\n",
    "\n",
    "**Ostrzeżenie**: jeśli zauważysz jakieś dziwne wyjątki, takie jak `cannot call from_pretrained na obiekcie None` gdzieś w twoim kodzie, zrestartuj środowisko używając: Runtime -> restart. Następnie uruchom komórki z kodem (bez ponownej instalacji bibliotek) jeszcze raz."
   ],
   "metadata": {
    "id": "5sPUDVw7fKHJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ],
   "metadata": {
    "id": "jmRzPimhfRja",
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-04-30T10:35:36.442830700Z",
     "start_time": "2023-04-30T10:35:33.188930900Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: requests in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mifel\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mifel\\scoop\\apps\\python\\current\\lib\\site-packages (0.1.98)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Przeczytaj dokumentację dotyczącą modelu T5 dostępne tutaj: https://huggingface.co/docs/transformers/model_doc/t5\n",
    "\n",
    "W sekcji `wnioskowanie` znajdziesz opis pokazujący w jaki sposób możemy pobrać wytrenowany model i użyć go do rozwiązania zadanego zadania. Po prostu użyj dostarczonego kodu, aby przetłumaczyć jakieś zdanie z angielskiego na niemiecki!\n",
    "\n",
    "*(0.5 punkta)*"
   ],
   "metadata": {
    "id": "wSAg0LOEgGj4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lYOfsrkeJ6GF",
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-04-30T10:37:03.326492500Z",
     "start_time": "2023-04-30T10:37:00.691709300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dies ist ein Testsatz, der ins Deutsche übersetzt werden soll.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=1024)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "english_sentence = \"This is a test sentence to be translated into German.\"\n",
    "\n",
    "inputs = tokenizer.encode(\"translate English to German: \" + english_sentence, return_tensors='pt')\n",
    "\n",
    "outputs = model.generate(inputs, max_length=50, num_beams=4, early_stopping=True)\n",
    "german_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(german_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Różne zadania\n",
    "\n",
    "Eksperymentuj z innymi danymi wejściowymi, np. tymi, które przedstawiono na rysunku 1 przedstawionym w artykule przedstawiającym model T5 lub nawet szerszą listą przypadków użycia z Dodatku D dostarczonego z artykułem. Artykuł można znaleźć tutaj: https://arxiv.org/pdf/1910.10683.pdf\n",
    "\n",
    "Uwaga: wśród dostarczonych danych wejściowych zastosowano kilka skrótów, niektóre z nich to:\n",
    "- `stsb`: oznacza semantyczny test porównawczy podobieństwa tekstu. Biorąc pod uwagę dwa zdania, możemy obliczyć ich podobieństwa semantyczne, co może pomóc nam ustalić, czy jedno zdanie jest parafrazą drugiego.\n",
    "- `cola`: oznacza Corpus of Linguistic Acceptability i pomaga nam określić, czy dane zdanie jest gramatyczne, czy niegramatyczne.\n",
    "\n",
    "Jeśli spojrzysz na Dodatek D, skrótów jest więcej, są one związane z nazwami zadań przedstawionymi w benchmarku GLUE (dostępny tutaj: https://gluebenchmark.com/tasks) i benchmarku SUPERGLUE (dostępny tutaj: https:/ /super.gluebenchmark.com/tasks). Ideą GLUE i SUPERGLUE jest zebranie zestawu trudnych zadań, które można wykorzystać do oceny systemów wymagających rozumienia języka naturalnego.\n",
    "\n",
    "**Wklej 3 przykładowe zadania i przetworzone dane wejściowe w komórce poniżej (1 punkt)**"
   ],
   "metadata": {
    "id": "z322IklnhQnO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=1024)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Dane wejściowe dla zadania STSB\n",
    "print(\"Zadanie STSB (Semantic Textual Similarity Benchmark):\")\n",
    "\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "sentence2 = \"A brown fox isn't quick, and the dog is lazy.\"\n",
    "\n",
    "print(f\"Zdanie 1: {sentence1}\")\n",
    "print(f\"Zdanie 2: {sentence2}\")\n",
    "\n",
    "input_str = f\"stsb sentence1: {sentence1} sentence2: {sentence2}\"\n",
    "input_ids = tokenizer.encode(input_str, return_tensors='pt')\n",
    "\n",
    "# Generowanie odpowiedzi\n",
    "outputs = model.generate(input_ids=input_ids,\n",
    "                         max_length=128,\n",
    "                         num_beams=4,\n",
    "                         early_stopping=True)\n",
    "similarity_score_str = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "try:\n",
    "    similarity_score = float(similarity_score_str)\n",
    "    print(f\"Podobieństwo semantyczne między zdaniem 1 i zdaniem 2 wynosi: {similarity_score}\")\n",
    "except ValueError:\n",
    "    print(\"Nie udało się wyznaczyć podobieństwa semantycznego.\")\n",
    "\n",
    "# Dane wejściowe dla zadania CoLA\n",
    "print(\"\\nZadanie CoLA (Corpus of Linguistic Acceptability):\")\n",
    "\n",
    "sentence = \"The cat is on the mat.\"\n",
    "\n",
    "input_str = f\"cola sentence: {sentence}\"\n",
    "input_ids = tokenizer.encode(input_str, return_tensors='pt')\n",
    "\n",
    "print(sentence)\n",
    "\n",
    "# Generowanie odpowiedzi\n",
    "outputs = model.generate(input_ids=input_ids,\n",
    "                         max_length=128,\n",
    "                         num_beams=4,\n",
    "                         early_stopping=True)\n",
    "acceptability_label_str = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "if acceptability_label_str.lower() == \"unacceptable\":\n",
    "    print(\"Zdanie jest niepoprawne gramatycznie.\")\n",
    "elif acceptability_label_str.lower() == \"acceptable\":\n",
    "    print(\"Zdanie jest poprawne gramatycznie.\")\n",
    "else:\n",
    "    print(\"Nie udało się wyznaczyć poprawności gramatycznej zdania.\")\n",
    "\n",
    "# Dane wejściowe dla zadania MRPC\n",
    "print(\"\\nZadanie MRPC (Microsoft Research Paraphrase Corpus):\")\n",
    "\n",
    "sentence1 = \"The company has been accused of fraudulent activity by the SEC.\"\n",
    "sentence2 = \"The SEC has made an accusation of fraudulent activity against the company.\"\n",
    "\n",
    "print(f\"Zdanie 1: {sentence1}\")\n",
    "print(f\"Zdanie 2: {sentence2}\")\n",
    "\n",
    "input_str = f\"mrpc sentence1: {sentence1} sentence2: {sentence2}\"\n",
    "input_ids = tokenizer.encode(input_str, return_tensors='pt')\n",
    "\n",
    "# Generowanie odpowiedzi\n",
    "outputs = model.generate(input_ids=input_ids,\n",
    "                         max_length=128,\n",
    "                         num_beams=4,\n",
    "                         early_stopping=True)\n",
    "inference_label_str = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "if inference_label_str.lower() == \"not_equivalent\":\n",
    "    print(\"Zdania różnią się w znaczeniu.\")\n",
    "elif inference_label_str.lower() == \"equivalent\":\n",
    "    print(\"Zdania są podobne w znaczeniu.\")\n",
    "else:\n",
    "    print(\"Nie udało się wyznaczyć podobieństwa semantycznego między zdaniami.\")"
   ],
   "metadata": {
    "id": "OfxmQwV4lIXw",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zadanie STSB (Semantic Textual Similarity Benchmark):\n",
      "Zdanie 1: The quick brown fox jumps over the lazy dog.\n",
      "Zdanie 2: A brown fox isn't quick, and the dog is lazy.\n",
      "Podobieństwo semantyczne między zdaniem 1 i zdaniem 2 wynosi: 3.0\n",
      "\n",
      "Zadanie CoLA (Corpus of Linguistic Acceptability):\n",
      "The cat is on the mat.\n",
      "Zdanie jest poprawne gramatycznie.\n",
      "\n",
      "Zadanie MRPC (Microsoft Research Paraphrase Corpus):\n",
      "Zdanie 1: The company has been accused of fraudulent activity by the SEC.\n",
      "Zdanie 2: The SEC has made an accusation of fraudulent activity against the company.\n",
      "Zdania są podobne w znaczeniu.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Różne typy modeli\n",
    "\n",
    "Dostępnych jest kilka modeli T5, które różnią się wielkością (i jakością). Im większy model, tym lepsze wyjście powinien generować. Eksperymentuj z niektórymi modelami z następującego zestawu:\n",
    "- t5-small\n",
    "- base t5\n",
    "- t5-large\n",
    "- t5-3b\n",
    "- t5-11b\n",
    "\n",
    "Sprawdź, czy można zaobserwować różnicę w jakości generowanych tekstów.\n",
    "\n",
    "Porównaj również rozmiar modeli, możesz użyć funkcji `model.num_parameters()`, aby uzyskać numer parametru związany z każdym modelem. Dla każdego modelu, który jesteś w stanie załadować, podaj rozmiar w poniższej komórce (jeśli nie możesz załadować danego modelu, bo jest za duży, bez obaw, po prostu wpisz 'too big to load'). (*2 punkty*) "
   ],
   "metadata": {
    "id": "xQvUkC-BlW3q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# t5-small params number: 60506624\n",
    "# t5-base params number: 222903552\n",
    "# t5-large params number: 737668096\n",
    "# t5-3b params number: 2851598336\n",
    "# t5-11b params number: too big to load\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# t5-small\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', model_max_length=1024)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "print(\"t5-small params number: \", model.num_parameters())\n",
    "\n",
    "# t5-base\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=1024)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "print(\"t5-base params number: \", model.num_parameters())\n",
    "\n",
    "# t5-large\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large', model_max_length=1024)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "print(\"t5-large params number: \", model.num_parameters())\n",
    "\n",
    "# t5-3b\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-3b', model_max_length=1024)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-3b')\n",
    "print(\"t5-3b params number: \", model.num_parameters())\n",
    "\n",
    "# t5-11b\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-11b', model_max_length=1024)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-11b')\n",
    "print(\"t5-11b params number: \", model.num_parameters())\n"
   ],
   "metadata": {
    "id": "1MTMhyyeR3ZJ",
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-04-30T16:10:47.469649100Z",
     "start_time": "2023-04-30T16:09:47.553761900Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5-small params number:  60506624\n",
      "t5-base params number:  222903552\n",
      "t5-large params number:  737668096\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 16777216 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 25\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# t5-3b\u001B[39;00m\n\u001B[0;32m     24\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m T5Tokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt5-3b\u001B[39m\u001B[38;5;124m'\u001B[39m, model_max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1024\u001B[39m)\n\u001B[1;32m---> 25\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mT5ForConditionalGeneration\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mt5-3b\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt5-3b params number: \u001B[39m\u001B[38;5;124m\"\u001B[39m, model\u001B[38;5;241m.\u001B[39mnum_parameters())\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# t5-11b\u001B[39;00m\n",
      "File \u001B[1;32m~\\scoop\\apps\\python\\current\\Lib\\site-packages\\transformers\\modeling_utils.py:2629\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   2626\u001B[0m     init_contexts\u001B[38;5;241m.\u001B[39mappend(init_empty_weights())\n\u001B[0;32m   2628\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ContextManagers(init_contexts):\n\u001B[1;32m-> 2629\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2631\u001B[0m \u001B[38;5;66;03m# Check first if we are `from_pt`\u001B[39;00m\n\u001B[0;32m   2632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_keep_in_fp32_modules:\n",
      "File \u001B[1;32m~\\scoop\\apps\\python\\current\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1549\u001B[0m, in \u001B[0;36mT5ForConditionalGeneration.__init__\u001B[1;34m(self, config)\u001B[0m\n\u001B[0;32m   1547\u001B[0m decoder_config\u001B[38;5;241m.\u001B[39mis_encoder_decoder \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1548\u001B[0m decoder_config\u001B[38;5;241m.\u001B[39mnum_layers \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mnum_decoder_layers\n\u001B[1;32m-> 1549\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder \u001B[38;5;241m=\u001B[39m \u001B[43mT5Stack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdecoder_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshared\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1551\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(config\u001B[38;5;241m.\u001B[39md_model, config\u001B[38;5;241m.\u001B[39mvocab_size, bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1553\u001B[0m \u001B[38;5;66;03m# Initialize weights and apply final processing\u001B[39;00m\n",
      "File \u001B[1;32m~\\scoop\\apps\\python\\current\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:881\u001B[0m, in \u001B[0;36mT5Stack.__init__\u001B[1;34m(self, config, embed_tokens)\u001B[0m\n\u001B[0;32m    877\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_tokens \u001B[38;5;241m=\u001B[39m embed_tokens\n\u001B[0;32m    878\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_decoder \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mis_decoder\n\u001B[0;32m    880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblock \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mModuleList(\n\u001B[1;32m--> 881\u001B[0m     \u001B[43m[\u001B[49m\u001B[43mT5Block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhas_relative_attention_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    882\u001B[0m )\n\u001B[0;32m    883\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinal_layer_norm \u001B[38;5;241m=\u001B[39m T5LayerNorm(config\u001B[38;5;241m.\u001B[39md_model, eps\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mlayer_norm_epsilon)\n\u001B[0;32m    884\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mDropout(config\u001B[38;5;241m.\u001B[39mdropout_rate)\n",
      "File \u001B[1;32m~\\scoop\\apps\\python\\current\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:881\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    877\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_tokens \u001B[38;5;241m=\u001B[39m embed_tokens\n\u001B[0;32m    878\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_decoder \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mis_decoder\n\u001B[0;32m    880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblock \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mModuleList(\n\u001B[1;32m--> 881\u001B[0m     [\u001B[43mT5Block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhas_relative_attention_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config\u001B[38;5;241m.\u001B[39mnum_layers)]\n\u001B[0;32m    882\u001B[0m )\n\u001B[0;32m    883\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinal_layer_norm \u001B[38;5;241m=\u001B[39m T5LayerNorm(config\u001B[38;5;241m.\u001B[39md_model, eps\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mlayer_norm_epsilon)\n\u001B[0;32m    884\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mDropout(config\u001B[38;5;241m.\u001B[39mdropout_rate)\n",
      "File \u001B[1;32m~\\scoop\\apps\\python\\current\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:655\u001B[0m, in \u001B[0;36mT5Block.__init__\u001B[1;34m(self, config, has_relative_attention_bias)\u001B[0m\n\u001B[0;32m    653\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_decoder \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mis_decoder\n\u001B[0;32m    654\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mModuleList()\n\u001B[1;32m--> 655\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer\u001B[38;5;241m.\u001B[39mappend(\u001B[43mT5LayerSelfAttention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhas_relative_attention_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_relative_attention_bias\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    656\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_decoder:\n\u001B[0;32m    657\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer\u001B[38;5;241m.\u001B[39mappend(T5LayerCrossAttention(config))\n",
      "File \u001B[1;32m~\\scoop\\apps\\python\\current\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:585\u001B[0m, in \u001B[0;36mT5LayerSelfAttention.__init__\u001B[1;34m(self, config, has_relative_attention_bias)\u001B[0m\n\u001B[0;32m    583\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, config, has_relative_attention_bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    584\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m--> 585\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mSelfAttention \u001B[38;5;241m=\u001B[39m \u001B[43mT5Attention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhas_relative_attention_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_relative_attention_bias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    586\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm \u001B[38;5;241m=\u001B[39m T5LayerNorm(config\u001B[38;5;241m.\u001B[39md_model, eps\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mlayer_norm_epsilon)\n\u001B[0;32m    587\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mDropout(config\u001B[38;5;241m.\u001B[39mdropout_rate)\n",
      "File \u001B[1;32m~\\scoop\\apps\\python\\current\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:364\u001B[0m, in \u001B[0;36mT5Attention.__init__\u001B[1;34m(self, config, has_relative_attention_bias)\u001B[0m\n\u001B[0;32m    362\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_model, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minner_dim, bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    363\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_model, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minner_dim, bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m--> 364\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mv \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43md_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minner_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mo \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minner_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_model, bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    367\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_relative_attention_bias:\n",
      "File \u001B[1;32m~\\scoop\\apps\\python\\current\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001B[0m, in \u001B[0;36mLinear.__init__\u001B[1;34m(self, in_features, out_features, bias, device, dtype)\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_features \u001B[38;5;241m=\u001B[39m in_features\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_features \u001B[38;5;241m=\u001B[39m out_features\n\u001B[1;32m---> 96\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight \u001B[38;5;241m=\u001B[39m Parameter(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_features\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfactory_kwargs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m bias:\n\u001B[0;32m     98\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;241m=\u001B[39m Parameter(torch\u001B[38;5;241m.\u001B[39mempty(out_features, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfactory_kwargs))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 16777216 bytes."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## T5 specyficzne dla języka (ZADANIE OPCJONALNE – nie musisz tutaj podawać kodu)\n",
    "\n",
    "Istnieją nawet alternatywy dla oryginalnych modeli T5. Ponieważ model T5 był trenowany na języku angielskim, dostępne są modele specyficzne dla innych języków, np. polskiego (np. plT5 zaproponowany przez Allegro - https://huggingface.co/allegro/plt5-small). Polski model został przeszkolony do rozwiązywania zestawu zadań zebranych w benchmarku KLEJ, który stanowi polską analogię do benchmarku GLUE: https://klejbenchmark.com.\n",
    "\n",
    "Więcej szczegółów na temat plT5 można znaleźć w artykule badawczym: https://arxiv.org/pdf/2205.08808.pdf. Tabela 2 przedstawia przykładowe podpowiedzi, które można wykorzystać do rozwiązania niektórych zadań wymienionych w KLEJ.\n",
    "\n",
    "Możesz wyszukać alternatywę dla oryginalnego T5, na przykład tę związaną z Twoim językiem, i poeksperymentować z nią (**to zadanie nie jest obowiązkowe**)."
   ],
   "metadata": {
    "id": "yDud66l6msUI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# (OPTIONAL): If you want, experiment with some alternative models (like language-related, e.g., plT5 related to Polish)"
   ],
   "metadata": {
    "id": "KJZZUkduoEhZ",
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-04-30T10:35:41.025901500Z",
     "start_time": "2023-04-30T10:35:41.023602Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flan-T5\n",
    "\n",
    "Pod koniec 2022 roku zaproponowano ewolucję T5 o nazwie Flan-T5. Ten model jest również dostarczany przez bibliotekę transformatorów HuggingFace. Odwiedź tę stronę: https://huggingface.co/docs/transformers/model_doc/flan-t5, aby zobaczyć, jak możesz użyć tego modelu (wystarczy zmienić nazwę modelu, aby pobrać!).\n",
    "\n",
    "Flan-T5 jest znacznie potężniejszy niż T5. Możesz zajrzeć do Dodatku D zawartego w artykule opisującym Flan T5, aby zapoznać się z niektórymi formatami wejściowymi (monitami) i generowanymi wartościami. Artykuł jest tutaj: https://arxiv.org/pdf/1910.10683.pdf. Powinieneś skupić się na polach „przetworzonych danych wejściowych”, ponieważ są to reprezentacje używane przez model. Eksperymentuj z wybranymi zadaniami i sprawdź, czy możesz uzyskać takie same wyniki! W poniższym kodzie wklej kod ładujący model Flan-T5 i wykorzystujący go do rozwiązania wybranych zadań. (*1 punkt*)"
   ],
   "metadata": {
    "id": "e1hsTnxboIe8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "# Dane wejściowe dla zadania STSB\n",
    "print(\"Zadanie STSB (Semantic Textual Similarity Benchmark):\")\n",
    "\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "sentence2 = \"A brown fox isn't quick, and the dog is lazy.\"\n",
    "\n",
    "print(f\"Zdanie 1: {sentence1}\")\n",
    "print(f\"Zdanie 2: {sentence2}\")\n",
    "\n",
    "input_str = f\"stsb sentence1: {sentence1} sentence2: {sentence2}\"\n",
    "input_ids = tokenizer.encode(input_str, return_tensors='pt')\n",
    "\n",
    "# Generowanie odpowiedzi\n",
    "outputs = model.generate(input_ids=input_ids,\n",
    "                         max_length=128,\n",
    "                         num_beams=4,\n",
    "                         early_stopping=True)\n",
    "for i in range(0, len(outputs)):\n",
    "    print(f\"Odpowiedź {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}\")\n",
    "similarity_score_str = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "try:\n",
    "    similarity_score = float(similarity_score_str)\n",
    "    print(f\"Podobieństwo semantyczne między zdaniem 1 i zdaniem 2 wynosi: {similarity_score}\")\n",
    "except ValueError:\n",
    "    print(\"Nie udało się wyznaczyć podobieństwa semantycznego.\")\n",
    "\n",
    "# Dane wejściowe dla zadania CoLA\n",
    "print(\"\\nZadanie CoLA (Corpus of Linguistic Acceptability):\")\n",
    "\n",
    "sentence = \"The cat is on the mat.\"\n",
    "\n",
    "input_str = f\"cola sentence: {sentence}\"\n",
    "input_ids = tokenizer.encode(input_str, return_tensors='pt')\n",
    "\n",
    "print(sentence)\n",
    "\n",
    "# Generowanie odpowiedzi\n",
    "outputs = model.generate(input_ids=input_ids,\n",
    "                         max_length=128,\n",
    "                         num_beams=4,\n",
    "                         early_stopping=True)\n",
    "acceptability_label_str = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "if acceptability_label_str.lower() == \"unacceptable\":\n",
    "    print(\"Zdanie jest niepoprawne gramatycznie.\")\n",
    "elif acceptability_label_str.lower() == \"acceptable\":\n",
    "    print(\"Zdanie jest poprawne gramatycznie.\")\n",
    "else:\n",
    "    print(\"Nie udało się wyznaczyć poprawności gramatycznej zdania.\")\n",
    "\n",
    "# Dane wejściowe dla zadania MRPC\n",
    "print(\"\\nZadanie MRPC (Microsoft Research Paraphrase Corpus):\")\n",
    "\n",
    "sentence1 = \"The company has been accused of fraudulent activity by the SEC.\"\n",
    "sentence2 = \"The SEC has made an accusation of fraudulent activity against the company.\"\n",
    "\n",
    "print(f\"Zdanie 1: {sentence1}\")\n",
    "print(f\"Zdanie 2: {sentence2}\")\n",
    "\n",
    "input_str = f\"mrpc sentence1: {sentence1} sentence2: {sentence2}\"\n",
    "input_ids = tokenizer.encode(input_str, return_tensors='pt')\n",
    "\n",
    "# Generowanie odpowiedzi\n",
    "outputs = model.generate(input_ids=input_ids,\n",
    "                         max_length=128,\n",
    "                         num_beams=4,\n",
    "                         early_stopping=True)\n",
    "inference_label_str = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "if inference_label_str.lower() == \"not_equivalent\":\n",
    "    print(\"Zdania różnią się w znaczeniu.\")\n",
    "elif inference_label_str.lower() == \"equivalent\":\n",
    "    print(\"Zdania są podobne w znaczeniu.\")\n",
    "else:\n",
    "    print(\"Nie udało się wyznaczyć podobieństwa semantycznego między zdaniami.\")\n"
   ],
   "metadata": {
    "id": "ppbmGGqVqERf",
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-04-30T19:50:04.090866100Z",
     "start_time": "2023-04-30T19:49:59.166551200Z"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zadanie STSB (Semantic Textual Similarity Benchmark):\n",
      "Zdanie 1: The quick brown fox jumps over the lazy dog.\n",
      "Zdanie 2: A brown fox isn't quick, and the dog is lazy.\n",
      "Odpowiedź 0: sentence1: The quick brown fox jumps over the lazy dog sentence2: A brown fox isn't quick and the dog is lazy\n",
      "Nie udało się wyznaczyć podobieństwa semantycznego.\n",
      "\n",
      "Zadanie CoLA (Corpus of Linguistic Acceptability):\n",
      "The cat is on the mat.\n",
      "Nie udało się wyznaczyć poprawności gramatycznej zdania.\n",
      "\n",
      "Zadanie MRPC (Microsoft Research Paraphrase Corpus):\n",
      "Zdanie 1: The company has been accused of fraudulent activity by the SEC.\n",
      "Zdanie 2: The SEC has made an accusation of fraudulent activity against the company.\n",
      "Nie udało się wyznaczyć podobieństwa semantycznego między zdaniami.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (OPCJONALNIE) Dostrajanie\n",
    "\n",
    "Możesz nawet dostroić model T5/Flan-T5, aby rozwiązać wybrane zadanie. Możesz załadować istniejący model T5/Flan-T5, który jest już przeszkolony do rozwiązywania niektórych zadań, i wykorzystać moc 'transfery wiedzy', aby nauczyć go rozwiązywać różne zadania. Jest to o wiele lepsze niż trenowanie sieci od zera i powinno wymagać mniejszej liczby przykładów szkoleniowych.\n",
    "\n",
    "Faza dostrajania jest dość złożona. Jednak opis krok po kroku można znaleźć tutaj: https://www.philschmid.de/fine-tune-flan-t5\n",
    "\n",
    "Możesz spróbować dostroić wybrany model."
   ],
   "metadata": {
    "id": "vwyRmBx6q0iT"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "frt5p4E_rjGd",
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-04-30T10:35:41.032378100Z",
     "start_time": "2023-04-30T10:35:41.030463500Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  }
 ]
}
